## Datasets:
PLOS (downloaded from http://www.nactem.ac.uk/readability/) and eLife (downloaded from https://github.com/TGoldsack1/Corpora_for_Lay_Summarisation).
Please put them into the `datasets` folder.

Then, use `data_process.ipynb` to process each dataset and obtain the corresponding data formats needed for each part (DIT/LPS/DCD).

## QA Pairs:
We provide 100 samples for each type of summaries on each dataset. (TODO: the whole doc will be released later...)

Include the `GPT-4-Turbo` generated golden QA pairs and our `QA pairs generator` predicted QA pairs.

## Long context Model:
Please download the model `LongAlpaca-7B` from https://huggingface.co/Yukang/LongAlpaca-7B, and then put it into the `base_model` folder.


## Checkpoint:
We provide a chackpoint of our instruction tuned model with the strategy DIT.

It should be noted that for rapid deployment and reproduction, the checkpoint is based on the 16K context window model `LongAlpaca-7B-16k`, which can achieve comparable performance while requiring less GPU memory.

(TODO: the formal version will be released later...)

## Some Results:
We provide part of our models' results and baseline results on test set for a clearer comparison in `model_results`. 

Note that: the reference summaries for the results (we provide here) generated by DEMO_DCD model in eLife is from `model_results/elife/test_filter_elife.jsonl`. Others are top 100 instances of test set on different datasets.

## Evaluation Metrics:
Please refer to the official code.

ROUGE 1/2/L, BERTScore and BARTScore: https://github.com/TGoldsack1/BioLaySumm2023-evaluation_scripts

Mint Score: https://github.com/amazon-science/abstractive-factual-tradeoff

## Acknowledgements

The work of this project has been inspired and assisted by the following open-source projects and technologies. We would like to express our gratitude to the developers and contributors of these projects, including but not limited to:

* llamafactory: https://github.com/hiyouga/LLaMA-Factory
* longlora: https://github.com/dvlab-research/LongLoRA
* context aware decoding: https://github.com/zhichaoxu-shufe/context-aware-decoding-qfs

  
## Code Structure 
```
--- datasets
    |--- plos
        |--- train_plos.jsonl
        |--- dev_plos.jsonl
        |--- test_plos.jsonl
    |--- elife
        |--- train.json
        |--- val.json
        |--- test.json
--- qa_pairs
    |--- gold
        |--- elife
            |--- pls_qa.txt (plain)
            |--- exp_qa.txt (technical)
        |--- plos
            |--- pls_qa.txt
            |--- exp_qa.txt
    |--- predict
        |--- elife
            |--- pls_qa.txt
            |--- exp_qa.txt
        |--- plos
            |--- pls_qa.txt
            |--- exp_qa.txt
--- data_process.ipynb
--- base_model
--- checkpoint
--- model_results
--- DIT
--- LPS
--- DCD
--- README.md
--- requirements.txt
```
