## Datasets:
PLOS (downloaded from http://www.nactem.ac.uk/readability/) and eLife (downloaded from https://github.com/TGoldsack1/Corpora_for_Lay_Summarisation).
Please put them into the `datasets` folder.

Then, use `data_process.ipynb` to process each dataset and obtain the corresponding data formats needed for each part (DIT/LPS/DCD).

## QA Pairs:
We provide 100 samples for each type of summaries on each dataset. (TODO: the whole doc will be released later...)

Include the `GPT-4-Turbo` generated golden QA pairs and our `QA pairs generator` predicted QA pairs.

## Long context Model:
Please download the model `LongAlpaca-7B` from https://huggingface.co/Yukang/LongAlpaca-7B, and then put it into the `base_model` folder.


## Checkpoint:
We provide a chackpoint of our instruction tuned model with the strategy DIT (https://huggingface.co/hanqinyu/DEMO).

It should be noted that for rapid deployment and reproduction, the checkpoint is based on the 16K context window model `LongAlpaca-7B-16k`, which can achieve comparable performance while requiring less GPU memory.

(TODO: the formal version will be released later...)


## Equipment:
All of our experiments are carried out using 4Ã— NVIDIA A40 48G GPUs.


## Some Results:
We provide part of our models' results and baseline results on test set for a clearer comparison in `model_results`. 

Note that: the reference summaries for the results (we provide here) generated by DEMO_DCD model in eLife is from `model_results/elife/test_filter_elife.jsonl`. Others are top 100 instances of test set on different datasets.

## Evaluation Metrics:
Please refer to the official code.

ROUGE 1/2/L, BERTScore and BARTScore: https://github.com/TGoldsack1/BioLaySumm2023-evaluation_scripts

Mint Score: https://github.com/amazon-science/abstractive-factual-tradeoff

## Acknowledgements

The work of this project has been inspired and assisted by the following open-source projects and technologies. We would like to express our gratitude to the developers and contributors of these projects, including but not limited to:

* llamafactory: https://github.com/hiyouga/LLaMA-Factory
* longlora: https://github.com/dvlab-research/LongLoRA
* context aware decoding: https://github.com/zhichaoxu-shufe/context-aware-decoding-qfs

## Citation

If you find our work useful, please consider citing it as:

```
@article{HAN2025130858,
title = {Reader Comes First: A Demand-Oriented Readability Controllable Summarization},
journal = {Expert Systems with Applications},
pages = {130858},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.130858},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425044732},
author = {Qinyu Han and Yuanyuan Sun and Zhihao Yang and Wenfei Liu and Ling Luo and Hongfei Lin},
keywords = {Controllable summarization, Document summarization, QA-assisted learning, Long-context modeling, Large language model},
}
```
